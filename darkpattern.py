# -*- coding: utf-8 -*-
"""darkpattern

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19E9AZawxNO5V4bhZishkLV_YgMgOInmW
"""

from transformers import TFAutoModel, AutoTokenizer
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras.models import Sequential
from keras.layers import LSTM, GRU,SimpleRNN
from keras.layers import Dense, Activation, Dropout
from keras.layers import Embedding
from keras.layers import BatchNormalization
# from keras.utils import np_utils
from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline
from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D
from keras.preprocessing import sequence, text
from keras.callbacks import EarlyStopping



import pandas as pd

# Load your TSV dataset
file_path = 'dataset.tsv'
df = pd.read_csv(file_path, sep='\t')

# Check for missing values in the entire dataset
missing_values_total = df.isnull().sum().sum()

# Check for missing values in each column
missing_values_per_column = df.isnull().sum()

# Check for missing values in each row
missing_values_per_row = df.isnull().sum(axis=1)

# Display the results
print(f"Total missing values in the dataset: {missing_values_total}")
print("\nMissing values per column:")
print(missing_values_per_column)
print("\nMissing values per row:")
print(missing_values_per_row)

label_counts = df['Pattern Category'].value_counts()

# Print the counts of unique labels
print("Count of Unique Labels:")
print(label_counts)

import pandas as pd
import pickle
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

# Load your TSV dataset
file_path = 'dataset.tsv'
df = pd.read_csv(file_path, sep='\t')

# Assuming your text data is in the 'text' column and labels in the 'label' column
X = df['text']
y = df['Pattern Category']

# Split the dataset into training and testing sets (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=23)

# Convert text data to numerical features using CountVectorizer
vectorizer = CountVectorizer()
X_train_vectorized = vectorizer.fit_transform(X_train)
X_test_vectorized = vectorizer.transform(X_test)

# pickle.dump(vectorizer, open('vec.pkl', 'wb') )
SVM = SVC(kernel='rbf', C=1.0, random_state=42)

random = RandomForestClassifier()
# Train a Decision Tree Classifier
# clf = DecisionTreeClassifier(random_state=42)
random.fit(X_train_vectorized, y_train)

# Make predictions on the test set
y_pred = random.predict(X_test_vectorized)

pickle.dump(random, open('random.pkl', 'wb'))

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Example: Predicting a new text
new_text = ["4 sold in last 5 hours"]
new_text_vectorized = vectorizer.transform(new_text)
prediction = random.predict(new_text_vectorized)
print(f"Prediction for the new text: {prediction[0]}")

from sklearn import preprocessing
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics import confusion_matrix, roc_auc_score
from sklearn.metrics import mean_squared_error
import numpy as np

# Calculation Model Loss


def loss(model, X_test, y_test):
    predictions = model.predict(X_test)
    mse = mean_squared_error(y_test, predictions)
    return mse


def print_score(clf, X_train, X_test, y_train, y_test, train=1):

    lb = preprocessing.LabelBinarizer()
    lb.fit(y_train)
    if train ==1:
        '''
        training performance
        '''
        res = clf.predict(X_train)
        print("Train Result:\n")
        print("accuracy score: {0:.4f}\n".format(accuracy_score(y_train,res)))


        print("Classification Report: \n {}\n".format(classification_report(y_train,res)))
        print("Confusion Matrix: \n {}\n".format(confusion_matrix(y_train,res)))
        print("ROC AUC: {0:.4f}\n".format(roc_auc_score(lb.transform(y_train),lb.transform(res))))

        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')
        print("Average Accuracy: \t {0:.4f}".format(np.mean(res)))
        print("Accuracy SD: \t\t {0:.4f}".format(np.std(res)))
        print("-------------------------------------------------------------------------")

    elif train== 0:
        '''
        test performance
        '''
        res_test = clf.predict(X_test)
        print("Test Result:\n")
        print("accuracy score: {0:.4f}\n".format(accuracy_score(y_test,res_test)))
        print("Testing Loss : ", loss(clf, X_test, y_test))

        print("Classification Report: \n {}\n".format(classification_report(y_test,
                                                                            res_test)))
        print("Confusion Matrix: \n {}\n".format(confusion_matrix(y_test,
                                                                  res_test)))
        print("ROC AUC: {0:.4f}\n".format(roc_auc_score(lb.transform(y_test),
                                                      lb.transform(res_test))))
        print("-------------------------------------------------------------------------")







# X_train_vectorized, X_test_vectorized,y_train_vectorized, y_test_vectorized

print_score(random, X_train_vectorized, X_test_vectorized, y_train, y_test, train=1)
print_score(random, X_train_vectorized, X_test_vectorized, y_train, y_test, train=0)

import pickle
# Save the model
with open("destree.pkl", "wb") as f:
    pickle.dump(clf, f)

def predict_from_model(pickle_file, text):
    """
    Loads a pickled model and makes a prediction on the given text.

    Args:
        pickle_file (str): Path to the pickled model file.
        text (str): The text to make a prediction on.

    Returns:
        object: The model's prediction.
    """
    # vectorizer = CountVectorizer()
    with open(pickle_file, 'rb') as f:
        model = pickle.load(f)
    text_vectorized = vectorizer.transform(text)
    prediction = model.predict(text_vectorized)  # Enclose text in a list for prediction


    return prediction

predict_from_model('destree.pkl', ['4 sold in 5'])

import pandas as pd

def predict_from_csv(pickle_file, csv_file):
    """
    Loads a pickled model and makes predictions on text data from a CSV file.

    Args:
        pickle_file (str): Path to the pickled model file.
        csv_file (str): Path to the CSV file containing text data.

    Returns:
        list: A list of model predictions for each text sample in the CSV.
    """

    with open(pickle_file, 'rb') as f:
        model = pickle.load(f)

    df = pd.read_csv(csv_file, sep = '\t')  # Load CSV data
    text_column = "text"  # Specify the column containing text data
    text_data = df[text_column].tolist()  # Extract text samples

    # Assuming vectorizer is saved with the model:
    # vectorizer = model['vectorizer']  # Load vectorizer from model
    text_vectorized = vectorizer.transform(text_data)

    predictions = model.predict(text_vectorized)

    return predictions



predict_from_csv('destree.pkl', 'test.tsv')

new_text = ["4 sold in last 5 hours", "We cannot guarantee door to door delivery for every customer."]

# Vectorize the new text using the same CountVectorizer
new_text_vectorized = vectorizer.transform(new_text)

# Make predictions on the new text
new_text_predictions = clf.predict(new_text_vectorized)

# Print the predictions
for text, prediction in zip(new_text, new_text_predictions):
    print(f"Text: {text}\nPrediction: {prediction}\n")

from sklearn.naive_bayes import MultinomialNB

nb_clf = MultinomialNB()
nb_clf.fit(X_train_vectorized, y_train)

# Make predictions on the test set for Naive Bayes
y_pred_nb = nb_clf.predict(X_test_vectorized)

# Evaluate the Naive Bayes model
accuracy_nb = accuracy_score(y_test, y_pred_nb)
print(f"Naive Bayes Accuracy: {accuracy_nb:.2f}")

# Print Naive Bayes prediction
# Example: Predicting a new text
new_text_for_nb = ["4 sold in last 5 hours"]
new_text_vectorized = vectorizer.transform(new_text_for_nb)
predictio = clf.predict(new_text_vectorized)
print(f"Prediction for the new text: {predictio[0]}")

"""BERT"""

!pip install tensorflow-text

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text

# Load model directly
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment-latest")
model = AutoModelForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment-latest")

def get_sentence_embeding(sentences):
    preprocessed_text = bert_preprocess(sentences)
    return bert_encoder(preprocessed_text)['pooled_output']

get_sentence_embeding([
    "500$ discount. hurry up",
    "Bhavin, are you up for a volleybal game tomorrow?"]
)

X_train.shape

# Bert layers
text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
preprocessed_text = bert_preprocess(text_input)
outputs = bert_encoder(preprocessed_text)

# Neural network layers
l = tf.keras.layers.Dropout(0.1, name="dropout")(outputs['pooled_output'])
l = tf.keras.layers.Dense(1, activation='sigmoid', name="output")(l)

# Use inputs and outputs to construct a final model
model = tf.keras.Model(inputs=[text_input], outputs = [l])

model.summary()

METRICS = [
      tf.keras.metrics.Accuracy(name='accuracy'),
      tf.keras.metrics.Precision(name='precision'),
      tf.keras.metrics.Recall(name='recall')
]

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=METRICS)

model.fit(X_train_vectorized, y_train, epochs=10)

import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, TFBertForSequenceClassification

# Load your TSV dataset
file_path = 'dataset.tsv'
df = pd.read_csv(file_path, sep='\t')

# Assuming your text data is in the 'text' column and labels in the 'label' column
X = df['text']
y = df['label']

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Load pre-trained BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # Assuming binary classification
# Load model directly
X_train_tokenized = tokenizer(X_train.tolist(), padding=True, truncation=True, return_tensors='tf', max_length=256)
X_test_tokenized = tokenizer(X_test.tolist(), padding=True, truncation=True, return_tensors='tf', max_length=256)

# Convert labels to tensors
y_train = tf.convert_to_tensor(y_train.values)
y_test = tf.convert_to_tensor(y_test.values)

# Extract input tensors from BatchEncoding
input_ids_train = X_train_tokenized['input_ids']
token_type_ids_train = X_train_tokenized['token_type_ids']
attention_mask_train = X_train_tokenized['attention_mask']

input_ids_test = X_test_tokenized['input_ids']
token_type_ids_test = X_test_tokenized['token_type_ids']
attention_mask_test = X_test_tokenized['attention_mask']

# Compile the model
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)
model.compile( loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(
    {'input_ids': input_ids_train, 'token_type_ids': token_type_ids_train, 'attention_mask': attention_mask_train},
    y_train,
    epochs=30,
    batch_size=32,
    validation_split=0.3
)

# Evaluate the model on the test set
results = model.evaluate(
    {'input_ids': input_ids_test, 'token_type_ids': token_type_ids_test, 'attention_mask': attention_mask_test},
    y_test
)
print("Test Set Accuracy:", results[1])

!pip install accelerate -U

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from sklearn.model_selection import train_test_split
import pandas as pd
import torch

# Load your TSV dataset
file_path = 'dataset.tsv'
df = pd.read_csv(file_path, sep='\t')

# Assuming your text data is in the 'text' column and labels in the 'label' column
X = df['text']
y = df['Pattern Category']

# Split the dataset into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Load the pre-trained DistilBERT tokenizer and model for sequence classification
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

# Tokenize the training and validation sets
train_encodings = tokenizer(list(X_train), truncation=True, padding=True, return_tensors="pt").to_dict()
val_encodings = tokenizer(list(X_val), truncation=True, padding=True, return_tensors="pt").to_dict()

# Convert labels to integers (assuming 'label' is categorical)
label_mapping = {label: idx for idx, label in enumerate(y.unique())}
y_train = torch.tensor([label_mapping[label] for label in y_train])
y_val = torch.tensor([label_mapping[label] for label in y_val])

# Define the training arguments
training_args = TrainingArguments(
    output_dir="./distilbert-finetuned",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
)

# Define the Trainer
train_dataset = torch.utils.data.TensorDataset(train_encodings["input_ids"], train_encodings["attention_mask"], y_train)
eval_dataset = torch.utils.data.TensorDataset(val_encodings["input_ids"], val_encodings["attention_mask"], y_val)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./distilbert-finetuned")
tokenizer.save_pretrained("./distilbert-finetuned")